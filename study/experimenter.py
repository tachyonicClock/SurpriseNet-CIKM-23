import time
import typing as t

import git
from config.config import ExpConfig
from train import Experiment

import vizier.pyvizier as vz
from vizier.benchmarks import experimenters
from vizier.client.client_abc import TrialInterface

ACC_TID = "TaskIdAccuracy"
ACC_FINAL = "Accuracy_On_Trained_Experiences/eval_phase/test_stream/Task000"
ACC_FINAL_SHORT = "Accuracy"


class SurpriseNetExperimenter(experimenters.Experimenter):
    def __init__(self, base_cfg: ExpConfig) -> None:
        super().__init__()
        self.cfg = base_cfg

    def _evaluate(self, cfg: ExpConfig, suggestion: TrialInterface):
        """Evaluates a single Trial."""
        # Log the hyperparameters
        print("=-" * 40)
        for dotpath, value in suggestion.parameters.items():
            print(f"  {dotpath}: {value}")
        print("=-" * 40)

        # Create an experiment incorporating the suggested hyperparameters
        for dotpath, value in suggestion.parameters.items():
            cfg.set_dotpath(dotpath, value)
        exp = Experiment(cfg)

        # Add metadata to the trial
        repo = git.Repo(search_parent_directories=True)
        metadata = vz.Metadata(
            {
                "ExperimentLabel": exp.label,
                "GitCommit": repo.head.object.hexsha,
                "GitCommitMessage": repo.head.object.message,
                "GitCommitAuthor": repo.head.object.author.name,
                "GitCommitDate": str(repo.head.object.authored_datetime.isoformat()),
                "GitIsDirty": str(repo.is_dirty()),
            }
        )
        suggestion.update_metadata(metadata)

        # Add the hyperparameters to TensorBoard
        exp.logger.writer.add_hparams(
            suggestion.parameters,
            {ACC_TID: 0.0, ACC_FINAL: 0.0},
        )
        final_measurement = vz.Measurement({ACC_TID: 0.0, ACC_FINAL_SHORT: 0.0})

        start_time = time.time()
        try:
            result = exp.train()
        except Exception as e:
            print()
            print(f"{cfg.name} failed")
            print(e)

            final_measurement.elapsed_secs = time.time() - start_time
            suggestion.complete(
                final_measurement, infeasible_reason="Error during training"
            )
            return

        final_measurement.elapsed_secs = time.time() - start_time
        final_measurement.metrics[ACC_TID] = result[-1][ACC_TID]
        final_measurement.metrics[ACC_FINAL_SHORT] = result[-1][ACC_FINAL]
        suggestion.complete(final_measurement)

    def evaluate(self, suggestions: t.Sequence[vz.Trial]):
        """Evaluates and mutates the Trials in-place."""
        for suggestion in suggestions:
            self._evaluate(self.cfg.copy(), suggestion)

    def problem_statement(self) -> vz.ProblemStatement:
        """The search configuration generated by this experimenter."""

        problem = vz.ProblemStatement()
        search = problem.search_space.root

        # SEARCH SPACE --------------------------------------------------------
        search.add_discrete_param("network_cfg.layer_growth", [1.0])
        search.add_discrete_param("batch_size", [500])
        search.add_float_param("learning_rate", 1e-6, 0.001)
        search.add_float_param("network_cfg.dropout", 0.1, 0.5)
        search.add_int_param("latent_dims", 8, 256)
        search.add_float_param("prune_proportion", 0.6, 0.8)

        # OBJECTIVES ----------------------------------------------------------
        problem.metric_information.append(
            vz.MetricInformation(name=ACC_TID, goal=vz.ObjectiveMetricGoal.MAXIMIZE)
        )

        return problem

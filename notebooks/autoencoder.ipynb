{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from https://pytorch-lightning.readthedocs.io/en/latest/notebooks/course_UvA-DL/08-deep-autoencoders.html\n",
    "import os\n",
    "from urllib.error import HTTPError\n",
    "import typing\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../datasets\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"saved_models/autoencoder\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.RandomHorizontalFlip(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = FashionMNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [int(len(train_dataset)*0.9), int(len(train_dataset)*0.1)])\n",
    "\n",
    "print(f\"val-set size:   {len(val_set)}\")\n",
    "print(f\"train-set size: {len(train_set)}\")\n",
    "\n",
    "\n",
    "# Loading the test set\n",
    "test_set = FashionMNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=1024, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=1024, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "def get_train_images(num):\n",
    "    return torch.stack([val_set[i][0] for i in range(num)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "            nn.Unflatten(1, (1, 28, 28))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def Head(latent_dim: int):\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(latent_dim, 10),\n",
    "        nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        latent_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = Encoder(input_size, latent_dim)\n",
    "        self.decoder = Decoder(input_size, latent_dim)\n",
    "        self.valid_acc = torchmetrics.Accuracy()\n",
    "\n",
    "        self.head = Head(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        y_hat = self.head.forward(z)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, y_hat, z\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch: typing.Tuple[Tensor, Tensor], forward_out: typing.Tuple[Tensor, Tensor, Tensor]):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case)\"\"\"\n",
    "        x, y = batch\n",
    "        x_hat, y_hat, z = forward_out\n",
    "\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum().mean()\n",
    "\n",
    "        return loss + F.cross_entropy(y_hat, y).sum().mean() + z.abs().sum().mean()\n",
    "\n",
    "    # def _get_diversity_loss(self, batch):\n",
    "        # Run batch on all other ensembles\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3, weight_decay=0.001)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch, self.forward(batch[0]))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x_hat, y_hat, z = self.forward(x)\n",
    "        loss = self._get_reconstruction_loss(batch, (x_hat, y_hat, z))\n",
    "\n",
    "        self.valid_acc(y_hat, y)\n",
    "\n",
    "        self.log(\"z_active\", z.abs().sum().mean())\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log('valid_acc', self.valid_acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch, self.forward(batch[0]))\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(pl.Callback):\n",
    "    def __init__(self, input_imgs, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "        # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "\n",
    "    def on_epoch_end(self, trainer: pl.Trainer, pl_module: Autoencoder):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Reconstruct images\n",
    "            input_imgs = self.input_imgs.to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                reconst_imgs, y_hat, z = pl_module(input_imgs)\n",
    "                pl_module.train()\n",
    "            # Plot and add to tensorboard\n",
    "            tensorboard: SummaryWriter = trainer.logger.experiment\n",
    "            imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1, 1))\n",
    "            tensorboard.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fmnist(latent_dim):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"fmnist_%i\" % latent_dim),\n",
    "        gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
    "        max_epochs=500,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True),\n",
    "            GenerateCallback(get_train_images(10), every_n_epochs=10),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "    )\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"fmnist_%i.ckpt\" % latent_dim)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = Autoencoder(input_size=28*28, latent_dim=latent_dim)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result, \"val\": val_result}\n",
    "    print(pretrained_filename, result)\n",
    "    return f\"fmnist_{latent_dim:03d}\", model, result\n",
    "\n",
    "\n",
    "my_models = {}\n",
    "def save_model(name, model, result):\n",
    "    my_models[name] = (model, result)\n",
    "\n",
    "# train_fmnist(8)\n",
    "train_fmnist(128)\n",
    "# train_fmnist(32)\n",
    "# train_fmnist(64)\n",
    "# train_fmnist(128)\n",
    "# train_fmnist(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode entirety of FMNIST to latent space codes\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dill\n",
    "\n",
    "class EncodedFashionMNIST():\n",
    "\n",
    "    class LabeledCodes(Dataset):\n",
    "        latent_codes: Tensor\n",
    "        targets: Tensor\n",
    "\n",
    "        def __init__(self, encoder: nn.Module, dataloader: DataLoader):\n",
    "            for i, (x, y) in enumerate(dataloader):\n",
    "                with torch.no_grad():\n",
    "                    latent_code: Tensor = encoder(x)\n",
    "                if i == 0:\n",
    "                    self.latent_codes = latent_code\n",
    "                    self.targets = y\n",
    "                else:\n",
    "                    self.latent_codes = torch.cat((self.latent_codes, latent_code))\n",
    "                    self.targets = torch.cat((self.targets, y))\n",
    "\n",
    "            print(self.latent_codes.size())\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.targets)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return (self.latent_codes[index], self.targets[index])\n",
    "\n",
    "    testset: LabeledCodes \n",
    "    trainset: LabeledCodes\n",
    "\n",
    "    def __init__(self, encoder, train_loader, test_loader) -> None:\n",
    "        self.testset = EncodedFashionMNIST.LabeledCodes(encoder, test_loader)\n",
    "        self.trainset = EncodedFashionMNIST.LabeledCodes(encoder, train_loader)\n",
    "\n",
    "\n",
    "def save_AE_codes(checkpoint, save_path):\n",
    "    model = Autoencoder.load_from_checkpoint(checkpoint)\n",
    "\n",
    "    train_encoder_loader = data.DataLoader(train_dataset, batch_size=256, pin_memory=True, num_workers=4, shuffle=False)\n",
    "    test_encoder_loader = data.DataLoader(test_set, batch_size=256, pin_memory=True, num_workers=4)\n",
    "\n",
    "    codes_dataset = EncodedFashionMNIST(model.encoder, train_encoder_loader, test_encoder_loader)\n",
    "\n",
    "    with open(save_path, \"bw\") as f:\n",
    "        dill.dump(codes_dataset, f, recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 128])\n",
      "torch.Size([60000, 128])\n"
     ]
    }
   ],
   "source": [
    "# notebooks/\n",
    "save_AE_codes(\"saved_models/autoencoder/fmnist_128/lightning_logs/version_3/checkpoints/epoch=499-step=25999.ckpt\",   \"L128_FMNIST_SSAE.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_AE_codes(\"saved_models/autoencoder/fmnist_8/lightning_logs/version_0/checkpoints/epoch=499-step=25999.ckpt\",   \"L008_FMNIST.pkl\")\n",
    "save_AE_codes(\"saved_models/autoencoder/fmnist_16/lightning_logs/version_0/checkpoints/epoch=499-step=25999.ckpt\",  \"L016_FMNIST.pkl\")\n",
    "save_AE_codes(\"saved_models/autoencoder/fmnist_32/lightning_logs/version_0/checkpoints/epoch=499-step=25999.ckpt\",  \"L032_FMNIST.pkl\")\n",
    "save_AE_codes(\"saved_models/autoencoder/fmnist_64/lightning_logs/version_0/checkpoints/epoch=499-step=25999.ckpt\",  \"L064_FMNIST.pkl\")\n",
    "save_AE_codes(\"saved_models/autoencoder/fmnist_128/lightning_logs/version_0/checkpoints/epoch=499-step=25999.ckpt\", \"L128_FMNIST.pkl\")\n",
    "save_AE_codes(\"saved_models/autoencoder/fmnist_256/lightning_logs/version_0/checkpoints/epoch=499-step=25999.ckpt\", \"L256_FMNIST.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4839fe716dff7faeda236a63134f028e1f3d97c1113b9276d429fcc26fd67641"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('avalanche-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Stolen from https://pytorch-lightning.readthedocs.io/en/latest/notebooks/course_UvA-DL/08-deep-autoencoders.html\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "# Tensorboard extension (for visualization purposes later)\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../datasets\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"saved_models/autoencoder\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val-set size:   6000\n",
      "train-set size: 54000\n"
     ]
    }
   ],
   "source": [
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = FashionMNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [int(len(train_dataset)*0.9), int(len(train_dataset)*0.1)])\n",
    "\n",
    "print(f\"val-set size:   {len(val_set)}\")\n",
    "print(f\"train-set size: {len(train_set)}\")\n",
    "\n",
    "\n",
    "# Loading the test set\n",
    "test_set = FashionMNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "def get_train_images(num):\n",
    "    return torch.stack([val_set[i][0] for i in range(num)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size: int, latent_dim: int, act_fn: object = nn.ReLU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, 512),\n",
    "            act_fn(),\n",
    "            nn.Linear(512, 128),\n",
    "            act_fn(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "            act_fn(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size: int, latent_dim: int, act_fn: object = nn.ReLU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            act_fn(),\n",
    "            nn.Linear(128, 512),\n",
    "            act_fn(),\n",
    "            nn.Linear(512, input_size),\n",
    "            nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "            nn.Unflatten(1, (1, 28, 28))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        latent_dim: int,\n",
    "        encoder_class: object = Encoder,\n",
    "        decoder_class: object = Decoder,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(input_size, latent_dim)\n",
    "        self.decoder = decoder_class(input_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case)\"\"\"\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum().mean()\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(pl.Callback):\n",
    "    def __init__(self, input_imgs, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "        # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Reconstruct images\n",
    "            input_imgs = self.input_imgs.to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                reconst_imgs = pl_module(input_imgs)\n",
    "                pl_module.train()\n",
    "            # Plot and add to tensorboard\n",
    "            imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1, 1))\n",
    "            trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fmnist(latent_dim):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"fmnist_%i\" % latent_dim),\n",
    "        gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
    "        max_epochs=500,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True),\n",
    "            GenerateCallback(get_train_images(10), every_n_epochs=10),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "    )\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"fmnist_%i.ckpt\" % latent_dim)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = Autoencoder(input_size=28*28, latent_dim=latent_dim)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result, \"val\": val_result}\n",
    "    return model, result\n",
    "train_fmnist(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 16])\n",
      "torch.Size([60000, 16])\n"
     ]
    }
   ],
   "source": [
    "# Encode entirety of FMNIST to latent space codes\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EncodedFashionMNIST():\n",
    "\n",
    "    class LabeledCodes(Dataset):\n",
    "        latent_codes: Tensor\n",
    "        targets: Tensor\n",
    "\n",
    "        def __init__(self, encoder: nn.Module, dataloader: DataLoader):\n",
    "            for i, (x, y) in enumerate(dataloader):\n",
    "                with torch.no_grad():\n",
    "                    latent_code: Tensor = encoder(x)\n",
    "                if i == 0:\n",
    "                    self.latent_codes = latent_code\n",
    "                    self.targets = y\n",
    "                else:\n",
    "                    self.latent_codes = torch.cat((self.latent_codes, latent_code))\n",
    "                    self.targets = torch.cat((self.targets, y))\n",
    "\n",
    "            print(self.latent_codes.size())\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.targets)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return (self.latent_codes[index], self.targets[index])\n",
    "\n",
    "    testset: LabeledCodes \n",
    "    trainset: LabeledCodes\n",
    "\n",
    "    def __init__(self, encoder, train_loader, test_loader) -> None:\n",
    "        self.testset = EncodedFashionMNIST.LabeledCodes(encoder, test_loader)\n",
    "        self.trainset = EncodedFashionMNIST.LabeledCodes(encoder, train_loader)\n",
    "\n",
    "model = Autoencoder.load_from_checkpoint(\"saved_models/autoencoder/fmnist_16/lightning_logs/version_10/checkpoints/epoch=368-step=77489.ckpt\")\n",
    "\n",
    "train_encoder_loader = data.DataLoader(train_dataset, batch_size=256, pin_memory=True, num_workers=4, shuffle=False)\n",
    "test_encoder_loader = data.DataLoader(test_set, batch_size=256, pin_memory=True, num_workers=4)\n",
    "\n",
    "my_dataset = EncodedFashionMNIST(model.encoder, train_encoder_loader, test_encoder_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0000, 3.1287, 6.3555, 0.0000, 8.0942, 0.0000, 4.8815, 0.0000, 3.5234,\n",
      "        3.7017, 6.4643, 3.2514, 4.7463, 0.0000, 2.4219, 7.5642]), tensor(9))\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "\n",
    "print(my_dataset.testset[0])\n",
    "with open(\"latent_fashion_mnist.pkl\", \"bw\") as f:\n",
    "    dill.dump(my_dataset, f, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks.generators import nc_benchmark\n",
    "\n",
    "\n",
    "nc_benchmark()\n",
    "\n",
    "\n",
    "\n",
    "# class LatentFashionMnist():"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4839fe716dff7faeda236a63134f028e1f3d97c1113b9276d429fcc26fd67641"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('avalanche-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
